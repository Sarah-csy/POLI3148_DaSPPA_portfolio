---
title: "Lecture10_note_text_mining"
author: "Shukyee Chan"
date: "2023-11-23"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, message = FALSE, warning = FALSE)
```

# 1. String Operations

## Load Packages

```{r, message=FALSE}
library(tidyverse) # Package for data work

library(knitr) # Package for data presentation at Rmarkdown
library(kableExtra)  # Package for data presentation at Rmarkdown
```

## Load Data

```{r, results="asis"}
d = read_rds("Lecture_10/data/data_ce_speech_article.rds")

dim(d) # Get dimension of dataset d
names(d) # Get column names of dataset d

# str(d) # chr 
# DT::datatable(d |> slice(1:5))
```

## `strings`

```{r}
str(d)
summary(d)
class(d$date_of_speech)
typeof(d$date_of_speech)
```

## Case 1: When are the speeches/articles delivered?

```{r}
# Take a look at at the date_of_speech variable
d |> select(date_of_speech)
# The variable "date_of_speech" is currently treated as a string.
# Valuable information cannot be obtained from it in its current form.
# The first task is to use R's string operations.
# The goal is to subtract the relevant information from this string-type date indicator.
```

### Subtract strings using locations with `str_sub`

```{r}
?str_sub
# str_sub() function extracts or replaces the elements at a single position in each string.
# str_sub_all() function allows you to extract strings at multiple elements in every string.
```

#### Case: Getting YEAR

```{r}
d |>
  select(date_of_speech) |>
  # create a new variable called YEAR, that gets the  the last but three to the last characters of the string
  mutate(YEAR = str_sub(date_of_speech, 
                        start = -4, end = -1))

#Key for position extraction:
#   start = -4 starts from the third position from the end.
#   end = -1 ends at the last position.
# Removing the negative sign specifies starting/ending positions within the string.
# Useful for extracting "day of the month" information.
```

#### Case: Getting DAY

```{r}
d |>
  select(date_of_speech) |>
  # DAY (of month): the first two characters of the strings
  mutate(DAY = str_sub(date_of_speech, start = 1, end = 2))
```

#### How can you get MONTH?

```{r, eval=FALSE}
# The table named d_date is designed to store specific information.
# It includes variables such as filename, date_of_speech, YEAR, MONTH, and DAY.
# The string operation str_sub is utilized for extracting relevant data from the date_of_speech variable.
# This extraction process enables populating the YEAR, MONTH, and DAY variables with the extracted information.
d |>
  select(date_of_speech) |>
  mutate(DAY = str_sub(date_of_speech, start = 4, end = -5))
```

```{r, echo=FALSE, results="asis"}
d_date = d |>
  select(filename, date_of_speech) |>
  mutate(
    YEAR = str_sub(date_of_speech, start = -4, end = -1),
    MONTH = str_sub(date_of_speech, start = 3, end = -5),
    DAY = str_sub(date_of_speech, start = 1, end = 2)
    )

head(d_date, 5) |> kable() |> kable_styling()
```

### Remove and replace

-   Functions for removing patterns from strings:

    -   `str_remove`: Removes a specific pattern from a string.

    -   `str_remove_all`: Removes all occurrences of a specific pattern from a string.

-   Functions for replacing patterns in strings:

    -   `str_replace`: Replaces a specific pattern in a string with another pattern.

    -   `str_replace_all`: Replaces all occurrences of a specific pattern in a string with another pattern.

#### Case: Removing the `.` in the DAY variables with `str_remove`

```{r}
# Try this first
d_date |>
  mutate(DAY_t = str_remove(DAY, "."))

# Problem:
#   In R, the dot "." is a special character that matches any character.
#   Using str_remove without escaping the dot will result in an error.
#   To remove the dot character specifically, it should be escaped using a backslash "\".
#   This ensures that R interprets it as a literal dot character, rather than a wildcard.
```

```{r}
d_date |>
  mutate(DAY_t = str_remove(DAY, "\\."))
```

#### Remove `.`'s in the `MONTH` variable using `str_remove_all`

```{r}
d_date |>
  mutate(MONTH_t = str_remove(MONTH, "\\."), .after = MONTH)

#Problem
# When using str_remove on the MONTH variable, there might be remaining dot "." characters.
# This is because str_remove only removes the first occurrence from left to right.
# To remove all occurrences of the dot character, use str_remove_all instead.
```

```{r}
d_date |>
  mutate(MONTH_t = str_remove_all(MONTH, "\\."), .after = MONTH)
```

#### Complete the cleaned data

-   Name the processed dataset `d_date_1`

-   Clean out the `.`'s

-   Convert `YEAR` `MONTH` `DAY` to numeric variables (hint: `as.numeric()`)

-   Optional: Provide summary statistics for `YEAR` `MONTH` `DAY` respectively

```{r}
d_date |>
  mutate(
    MONTH = str_remove_all(MONTH, "\\."),
    DAY = str_remove_all(DAY, "\\.")
  )

# Want to make it simpler? Use mutate_at
d_date |>
  mutate_at(vars(MONTH, DAY), ~str_remove_all(., "\\."))
```

### Replace patterns in strings

#### Case: Replace `.` by `-`

```{r}
# Replace . by - for the MONTH variable.
d_date |>
  mutate(MONTH = str_replace(MONTH, "\\.", "-"))

d_date |>
  mutate(MONTH = str_replace_all(MONTH, "\\.", "-"))
```

### Regular Expression

```{r}
# Example: Removing the ending .
d_date |>
  mutate(MONTH_t = str_remove(MONTH, "\\.$"))

# Example: Removing the starting .
d_date |>
  mutate(MONTH_t = str_remove(MONTH, "^\\."))
```

```{r}
# Check the matched pattern in the first three entries of MONTH
# As an aside: "$" following dataframe's name select the variable; [1:3] selects the first three elements
str_view_all(d_date$MONTH[1:3], "\\.$", html = TRUE)

# This function will be more handy when your text is more complicated.
str_view_all(d$title[1:3], "Article by CE:", html = TRUE)
```

1.  Replace the leading `.` of `MONTH` by 0
2.  Clean the `DAY` variable in the same way

```{r}
d_date |>
  mutate(
    MONTH = str_replace(MONTH, "^\\.", "0"),
    DAY = str_replace(DAY, "^\\.", "0")
  )

```

### Extract information from strings

```{r}
# Extract the first element found
d_date |>
  select(date_of_speech) |>
  mutate(date_of_speech_extract = 
           str_extract(date_of_speech, "[0-9]+"))

# Extract all the elements
d_date |>
  select(date_of_speech) |>
  mutate(date_of_speech_extract = str_extract_all(date_of_speech, "[0-9]+"))
```

-   `unnest_wider` is typically used to unnest columns that have the same number of elements.

-   `unnest_longer` is typically used to unnest outputs that have various number of elements.

```{r}
# unnest_longer
# When unsure about the number of elements extracted. 
d_date |>
  select(date_of_speech) |>
  mutate(date_of_speech_extract = str_extract_all(date_of_speech, "[0-9]+")) |>
  unnest_longer(date_of_speech_extract)


# unnest_wider
d_date |>
  select(date_of_speech) |>
  mutate(date_of_speech_extract = str_extract_all(date_of_speech, "[0-9]+")) |>
  unnest_wider(date_of_speech_extract, names_sep = "_")
```

### Merge and Split Strings `str_` 

-   Function that splits your strings using certain "separator": `str_split`

-   Function that merges your strings: `str_c`

```{r}
# Split the date variable using the separator
# Spearating the string using "." as spearators

d_date |>
   select(date_of_speech) |>
   mutate(
     date_of_speech_ex = str_split(date_of_speech, "\\.")
   )

d_date_result = d_date |>
  select(date_of_speech) |>
  mutate(
    date_of_speech_ex = str_split(date_of_speech, "\\.")
  ) |>
  unnest_wider(date_of_speech_ex, names_sep = "_")

```

```{r}
# Practice string merging with str_c
d_date_result

# Put together YEAR-MONTH-DAY

?str_c # paste, paste0

d_date_result |>
  mutate(
    # paste0
    date_merge = str_c(date_of_speech_ex_3, 
                       "-",
                       date_of_speech_ex_2, 
                       "-",
                       date_of_speech_ex_1
                       )
  )

d_date_result |>
  mutate(
    date_merge = str_c(date_of_speech_ex_3, 
                       "-",
                       str_pad(date_of_speech_ex_2, "0"), 
                       "-",
                       str_pad(date_of_speech_ex_1, "0")
                       )
  )
```

## Wrangling the Title

-   Separate speeches and articles

-   Get speeches' locations

-   Identify policy addresses

-   Identify COVID-related speeches and article

```{r}
# Separate articles and speech

# Generate an variable indicating whether a piece is an article or a speech
d_2 = d |>
  mutate(
    article = str_extract(title, "Article by CE"),
    speech = str_extract(title, "Speech by CE"),
    .after = uid
  )

# Filter only articles
d |>
  filter(str_detect(title, "Article by CE"))
```

# 2. Tokenization Exploratory Analysis

## Load package

```{r}
library(lubridate)
```

```{r}
d_fulltext = read_rds("Lecture_10/data/data_ce_speech_article.rds")
# Change the date variable to "date" format
d_fulltext = d_fulltext |> mutate(date_of_speech = dmy(date_of_speech))
```

## Tokenization

```{r}
library(tidytext)
```

```{r}
d_fulltext <- d_fulltext |>
  mutate(text = str_replace_all(text, "Hong Kong", "HongKong"))

d_tokenized = d_fulltext |>
  select(uid, date_of_speech, text) |>
  unnest_tokens(word, text)
# first arg: output name; second: input name

head(d_tokenized, 20)
```

## Wrangling: Remove Stop Words

```{r}
# Load Stopwords
data("stop_words")

head(stop_words, 20)
```

```{r}
# Remove stopwords
d_tokenized_s = d_tokenized |>
  anti_join(stop_words, by = "word")
# anti_join: whatever appearing in the stop_words dataframe, we remove it.
```

## Wrangling: Stemming

```{r}
library(SnowballC)
```

```{r}
d_tokenized_s = d_tokenized_s |>
  mutate(stem = wordStem(word))

head(d_tokenized_s, 20)
```

## Exploratory Data Analysis

### Count word frequencies

```{r}
# Count term frequencies (for raw words)
word_frequency = d_tokenized_s |>
  count(word, sort = TRUE)

head(word_frequency, 20)

# Count term frequencies (for Stemmed word -- recommended)
word_frequency = d_tokenized_s |>
  count(stem, sort = TRUE) |>
  rename("word" = "stem")

head(word_frequency, 20)
```

### Examine most popular words

```{r}
# Get a subset of most frequent words
word_frequency_top = word_frequency |>
  arrange(desc(n)) |> # Make sure that it is sorted properly
  slice(1:200) # Take the first 200 rows. 
```

### Plot most popular words

```{r}
word_frequency_top |>
  slice(1:10) |>
  mutate(word = reorder(word, n)) |>
  ggplot(aes(x = n, y = word)) +
  geom_col() +
  theme_bw()
```

### Plot a Word Cloud

```{r}
library(ggwordcloud)

word_frequency_top |>
  slice(1:100) |>
  ggplot(aes(label = word, size = n)) +
  scale_size_area(max_size = 14) +
  geom_text_wordcloud() +
  theme_minimal()
```

```{r}
# An alternative wordcloud package
library(wordcloud)

wordcloud(
  word_frequency_top$word, word_frequency_top$n, 
  rot.per = 0, random.order = FALSE, random.color = TRUE)
```

```{r, results='hide'}
library(wordcloud2)

wordcloud2(word_frequency_top)

wordcloud2(word_frequency_top, shape = "star")

wordcloud2(word_frequency_top, shape = "pentagon")
```

## Comparative Exploratory Analysis

```{r}
# The final set of analysis in this note specifically concentrates on conducting a comparative analysis of word frequencies.

# Calculate term frequencies for 2020 and 2021 respectively
word_frequency_compare_21_20 = 
  d_tokenized_s |>
  mutate(year = year(date_of_speech), .after = "date_of_speech") |>
  # Extract the year of the speech
  filter(year == 2020 | year == 2021) |>
  group_by(year, stem) |>
  count(sort = TRUE) |>
  pivot_wider(names_from = "year", values_from = "n", 
              names_prefix = "n_", values_fill = 0) |>
  ungroup() |>
  mutate(
    prop_2021 = n_2021 / sum(n_2021),
    prop_2020 = n_2020 / sum(n_2020)
  )
```

```{r}
# Visualize the word frequencies in the two years
word_frequency_compare_21_20 |>
  ggplot(aes(x = prop_2020, y = prop_2021)) +
  geom_point()

word_frequency_compare_21_20 |>
  ggplot(aes(x = prop_2020, y = prop_2021)) +
  geom_point() +
  scale_x_sqrt() + scale_y_sqrt()


word_frequency_compare_21_20 |>
  ggplot(aes(x = log(prop_2020), y = log(prop_2021))) +
  geom_point()

word_frequency_compare_21_20 |>
  filter(n_2020 >= 10) |>
  ggplot(aes(x = log(prop_2020), y = log(prop_2021))) +
  geom_point() +
  geom_smooth()
```

```{r}
# The biggest difference?

## What are the words that feature 2020 speeches
tmp_plot_20 = word_frequency_compare_21_20 |>
  mutate(diff = prop_2020 - prop_2021) |>
  slice_max(diff, n = 30) |>
  arrange(desc(diff))
  
## What are the words that feature 2021 speeches
tmp_plot_21 = word_frequency_compare_21_20 |>
  mutate(diff = prop_2021 - prop_2020) |>
  slice_max(diff, n = 30) |>
  arrange(desc(diff))
```

```{r}
# Visualize the difference in a nice way?
set.seed(327)
tmp_plot_merge = tmp_plot_21 |> 
  mutate(Year = "2021") |>
  bind_rows(
    tmp_plot_20 |> mutate(Year = "2020")
    ) 

tmp_plot_merge |>
  ggplot(aes(label = stem, x = Year, color = Year, size = abs(diff))) +
  geom_text_wordcloud_area() +
  scale_size_area(max_size = 14) +
  theme_minimal() +
  theme(legend.position = "top")

tmp_plot_merge |>
  ggplot(aes(label = stem, y = Year, color = Year, size = abs(diff))) +
  geom_text_wordcloud_area() +
  scale_size_area(max_size = 14) +
  theme_minimal() +
  theme(legend.position = "top")
```

# 3. Sentiment Analysis

## Sentiment Analysis

```{r}
library(textdata)
```

### Load Sentiment Dictionary

```{r}
dict_afinn = get_sentiments("afinn")
dict_bing = get_sentiments("bing")
dict_nrc = get_sentiments("nrc") 

table(dict_afinn$value)
table(dict_bing$sentiment)
table(dict_nrc$sentiment)
```

### Calculate the Simplest Sentiment Scores

```{r}
# Merge your tokenized documents with the sentiment dictionary
d_tokenized_s_afinn = d_tokenized_s |>
  select(uid, date_of_speech, word) |>
  inner_join(dict_afinn, by = "word")

# Aggregate the sentiment score for each document
d_tokenized_s_afinn_agg = d_tokenized_s_afinn |>
  group_by(uid, date_of_speech) |>
  summarise(sentiment_score = sum(value))

d_tokenized_s_afinn_agg = d_fulltext |>
  select(uid) |>
  left_join(d_tokenized_s_afinn_agg) |>
  mutate(sentiment_score = replace_na(sentiment_score, 0))

# Change of sentiment over time?
d_tokenized_s_afinn_agg |>
  ggplot(aes(x = date_of_speech, y = sentiment_score)) +
  geom_point(alpha = 0.6) +
  geom_smooth() +
  labs(
    title = "Sentiment Scores of Hong Kong CE's Speeches and Articles"
  ) +
  xlab("Date") + ylab("Sentiment Scores")
```

```{r}
# To do it better, we can normalize the sentiment scores by document lengths

# Merge your tokenized documents with the sentiment dictionary
d_tokenized_s_afinn = d_tokenized_s |>
  group_by(uid) |> mutate(doc_length = n()) |>
  ungroup() |>
  select(uid, date_of_speech, word, doc_length) |>
  inner_join(dict_afinn, by = "word") |>
  ungroup()

# Aggregate the sentiment score for each document
d_tokenized_s_afinn_agg = d_tokenized_s_afinn |>
  group_by(uid, date_of_speech) |>
  summarise(sentiment_score = sum(value) / mean(doc_length))

d_tokenized_s_afinn_agg = d_fulltext |>
  select(uid) |>
  left_join(d_tokenized_s_afinn_agg) |>
  mutate(sentiment_score = replace_na(sentiment_score, 0))

# Change of sentiment over time?
d_tokenized_s_afinn_agg |>
  ggplot(aes(x = date_of_speech, y = sentiment_score)) +
  geom_point(alpha = 0.6) +
  geom_smooth() +
  labs(
    title = "Sentiment Scores of Hong Kong CE's Speeches and Articles"
  ) +
  xlab("Date") + ylab("Sentiment Scores (Normalized)")
```

## Calculate Scores of Emotions

```{r}
dict_nrc

d_tokenized_s_nrc = d_tokenized_s |>
  inner_join(dict_nrc, by = "word", multiple = "all")

d_tokenized_s_nrc_agg = d_tokenized_s_nrc |>
  group_by(uid, date_of_speech, sentiment) |>
  count() |>
  pivot_wider(names_from = "sentiment", values_from = "n", 
              names_prefix = "sentiment_score_")

names(d_tokenized_s_nrc_agg)

# Change of sentiment over time?
d_tokenized_s_nrc_agg |>
  ggplot(aes(x = date_of_speech, y = sentiment_score_sadness)) +
  geom_point(alpha = 0.6) +
  geom_smooth() +
  labs(
    title = "Sentiment Scores of Hong Kong CE's Speeches and Articles"
  ) +
  xlab("Date") + ylab("Sadness Scores")
```

```{r}
# Normalize the sentiment scores
d_tokenized_s_nrc = d_tokenized_s |>
  group_by(uid) |>
  mutate(doc_length = n()) |>
  ungroup() |>
  inner_join(dict_nrc, by = "word", multiple = "all")

d_tokenized_s_nrc_agg = d_tokenized_s_nrc |>
  group_by(uid, date_of_speech, sentiment) |>
  summarise(n = n() / mean(doc_length)) |>
  pivot_wider(names_from = "sentiment", values_from = "n", 
              names_prefix = "sentiment_score_")


# Change of sentiment over time?
d_tokenized_s_nrc_agg |>
  ggplot(aes(x = date_of_speech, y = sentiment_score_sadness)) +
  geom_point(alpha = 0.6) +
  geom_smooth() +
  labs(
    title = "Sentiment Scores of Hong Kong CE's Speeches and Articles"
  ) +
  xlab("Date") + ylab("Sadness Scores (Normalized)")
```
